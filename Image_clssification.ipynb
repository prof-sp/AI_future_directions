{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "lG08X377w72D"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "i"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset (replace with your own recyclable dataset if available)\n",
        "(ds_train, ds_val), ds_info = tfds.load(\n",
        "    'rock_paper_scissors',\n",
        "    split=['train[:80%]', 'train[80%:]'],\n",
        "    as_supervised=True, with_info=True\n",
        ")\n",
        "\n",
        "IMG_SIZE = 160\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "def preprocess(image, label):\n",
        "    image = tf.image.resize(image, (IMG_SIZE, IMG_SIZE))\n",
        "    image = tf.cast(image, tf.float32) / 255.0\n",
        "    return image, label\n",
        "\n",
        "train = ds_train.map(preprocess).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "val   = ds_val.map(preprocess).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "KybR5pH9xM7t"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build lightweight MobileNetV2 model\n",
        "base_model = tf.keras.applications.MobileNetV2(\n",
        "    input_shape=(IMG_SIZE, IMG_SIZE, 3),\n",
        "    include_top=False,\n",
        "    weights='imagenet'\n",
        ")\n",
        "base_model.trainable = False\n",
        "\n",
        "inputs = tf.keras.Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n",
        "x = base_model(inputs, training=False)\n",
        "x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
        "outputs = tf.keras.layers.Dense(ds_info.features['label'].num_classes, activation='softmax')(x)\n",
        "\n",
        "model = tf.keras.Model(inputs, outputs)\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(train, validation_data=val, epochs=5)\n",
        "\n",
        "# Print accuracy metrics\n",
        "print(\"Training accuracy history:\", history.history['accuracy'])\n",
        "print(\"Validation accuracy history:\", history.history['val_accuracy'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ux4jqwNxM-9",
        "outputId": "564ad9ba-3afe-4075-e000-0db1775292bd"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 759ms/step - accuracy: 0.7346 - loss: 0.6649 - val_accuracy: 0.9881 - val_loss: 0.1035\n",
            "Epoch 2/5\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 737ms/step - accuracy: 0.9940 - loss: 0.0741 - val_accuracy: 0.9940 - val_loss: 0.0541\n",
            "Epoch 3/5\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 704ms/step - accuracy: 0.9958 - loss: 0.0393 - val_accuracy: 0.9980 - val_loss: 0.0352\n",
            "Epoch 4/5\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 724ms/step - accuracy: 0.9977 - loss: 0.0254 - val_accuracy: 0.9980 - val_loss: 0.0250\n",
            "Epoch 5/5\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 726ms/step - accuracy: 0.9984 - loss: 0.0179 - val_accuracy: 0.9980 - val_loss: 0.0189\n",
            "Training accuracy history: [0.8903769850730896, 0.9935516119003296, 0.9955357313156128, 0.9970238208770752, 0.9980158805847168]\n",
            "Validation accuracy history: [0.988095223903656, 0.9940476417541504, 0.9980158805847168, 0.9980158805847168, 0.9980158805847168]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import numpy as np\n",
        "\n",
        "val_images, val_labels = [], []\n",
        "for img, lbl in ds_val.map(preprocess):\n",
        "    val_images.append(img.numpy())\n",
        "    val_labels.append(lbl.numpy())\n",
        "val_images = np.stack(val_images)\n",
        "val_labels = np.array(val_labels)\n",
        "\n",
        "preds = model.predict(val_images)\n",
        "y_pred = np.argmax(preds, axis=1)\n",
        "\n",
        "print(confusion_matrix(val_labels, y_pred))\n",
        "print(classification_report(val_labels, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w4eXJEIwBLsx",
        "outputId": "ec8a6e45-06a0-4ff4-fb6d-e3255c7843e2"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 663ms/step\n",
            "[[157   0   0]\n",
            " [  0 171   1]\n",
            " [  0   0 175]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00       157\n",
            "           1       1.00      0.99      1.00       172\n",
            "           2       0.99      1.00      1.00       175\n",
            "\n",
            "    accuracy                           1.00       504\n",
            "   macro avg       1.00      1.00      1.00       504\n",
            "weighted avg       1.00      1.00      1.00       504\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load TFLite model for inference\n",
        "interpreter = tf.lite.Interpreter(model_path=\"classifier.tflite\")\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ex6MaLe81dot",
        "outputId": "57a3be32-a62e-4d02-e133-1033643eab2a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/tensorflow/lite/python/interpreter.py:457: UserWarning:     Warning: tf.lite.Interpreter is deprecated and is scheduled for deletion in\n",
            "    TF 2.20. Please use the LiteRT interpreter from the ai_edge_litert package.\n",
            "    See the [migration guide](https://ai.google.dev/edge/litert/migration)\n",
            "    for details.\n",
            "    \n",
            "  warnings.warn(_INTERPRETER_DELETION_WARNING)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test on one sample\n",
        "for image, label in ds_val.take(1):\n",
        "    img = tf.image.resize(image, (IMG_SIZE, IMG_SIZE))\n",
        "    img = tf.expand_dims(img, axis=0)  # batch dimension\n",
        "\n",
        "    interpreter.set_tensor(input_details[0]['index'], img)\n",
        "    interpreter.invoke()\n",
        "    prediction = interpreter.get_tensor(output_details[0]['index'])\n",
        "    print(\"True label:\", label.numpy(), \"Predicted:\", prediction.argmax())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SLgeHPeX1eB0",
        "outputId": "e3bc4ae1-ab13-4d84-cd4d-b2a0bc8c7a29"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True label: 2 Predicted: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "How Edge AI benefits real-time applications\n",
        "\n",
        "Reduced Latency: Inference happens locally on the device (e.g.,\n",
        "\n",
        "1.   List item\n",
        "2.   List item\n",
        "\n",
        "Raspberry Pi, smartphone) without sending data to the cloud. This means faster responses — crucial for tasks like sorting recyclables on a conveyor belt or detecting hazards in real time.\n",
        "\n",
        "Enhanced Privacy: Sensitive data (like images or health signals) stays on-device. Only results or summaries are sent to the cloud, reducing exposure risks.\n",
        "\n",
        "**Bandwidth Efficiency:** Instead of streaming raw video or sensor data, only metadata or alerts are transmitted, saving network resources.\n",
        "\n",
        "**Resilience: **Edge AI continues working even if internet connectivity drops — vital for rural or remote deployments.\n",
        "\n",
        "Example: A smart recycling bin with a Pi camera can classify items (plastic, paper, glass) locally. It immediately decides whether to accept or reject an item without needing to upload images to a server, ensuring both speed and privacy."
      ],
      "metadata": {
        "id": "HM01_zoD7d58"
      }
    }
  ]
}